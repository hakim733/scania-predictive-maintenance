{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakim733/scania-predictive-maintenance/blob/main/scania_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PckWiCX3J0vD",
        "outputId": "530c0aa0-5f8f-4a7d-cd45-447b4469aa76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18A955gJOHf0Ay3UComqy3SigMjf2X3Pi\n",
            "To: /content/train_specifications.csv\n",
            "100%|██████████| 1.08M/1.08M [00:00<00:00, 143MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WG7FYf9KaidtOaM04gBubkwYLi292Ken\n",
            "To: /content/train_tte.csv\n",
            "100%|██████████| 345k/345k [00:00<00:00, 118MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp\n",
            "From (redirected): https://drive.google.com/uc?id=1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp&confirm=t&uuid=a305dc4b-7a2f-4d98-8e55-6adabd6968d9\n",
            "To: /content/train_operational_readouts.csv\n",
            "100%|██████████| 1.22G/1.22G [00:16<00:00, 75.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tc-cAlepMP_EbObvxv0J6oqPH3u2DlLL\n",
            "To: /content/validation_specifications.csv\n",
            "100%|██████████| 232k/232k [00:00<00:00, 14.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc\n",
            "From (redirected): https://drive.google.com/uc?id=1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc&confirm=t&uuid=30cb5f69-2376-4dc7-82fc-132c7897bf66\n",
            "To: /content/validation_operational_readouts.csv\n",
            "100%|██████████| 216M/216M [00:04<00:00, 44.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lnd0VkDb2234csyl1OGt2jecUx6KPbuw\n",
            "To: /content/validation_labels.csv\n",
            "100%|██████████| 38.7k/38.7k [00:00<00:00, 38.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=138drtUXNmG4dwrJgIESUNZuW0aHKEPJU\n",
            "To: /content/test_specifications.csv\n",
            "100%|██████████| 232k/232k [00:00<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E\n",
            "From (redirected): https://drive.google.com/uc?id=1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E&confirm=t&uuid=23a3d6f9-490c-43fa-a190-db106ae4e664\n",
            "To: /content/test_operational_readouts.csv\n",
            "100%|██████████| 215M/215M [00:04<00:00, 52.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15njyTfKdErU3cvd837nuFoMBIgTtOOgv\n",
            "To: /content/test_labels.csv\n",
            "100%|██████████| 38.7k/38.7k [00:00<00:00, 51.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# 1. TRAINING DATA\n",
        "# ----------------------------\n",
        "# Training specifications\n",
        "file_id = '18A955gJOHf0Ay3UComqy3SigMjf2X3Pi'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_specifications.csv', quiet=False)\n",
        "train_specs = pd.read_csv(\"train_specifications.csv\")\n",
        "\n",
        "# Training labels (time-to-event)\n",
        "file_id = '1WG7FYf9KaidtOaM04gBubkwYLi292Ken'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_tte.csv', quiet=False)\n",
        "train_tte = pd.read_csv(\"train_tte.csv\")[['vehicle_id', 'in_study_repair','length_of_study_time_step']]\n",
        "\n",
        "# Training sensor data (critical fix: corrected file ID)\n",
        "file_id = '1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_operational_readouts.csv', quiet=False)\n",
        "train_ops = pd.read_csv(\"train_operational_readouts.csv\")\n",
        "# ----------------------------\n",
        "#  Merge training data\n",
        "# ----------------------------#\n",
        "train_df = train_ops.merge(train_tte, on='vehicle_id').merge(train_specs, on='vehicle_id')\n",
        "train_df.dropna(subset=['in_study_repair', 'length_of_study_time_step', 'time_step'], inplace=True)\n",
        "\n",
        "#----------------------------\n",
        "# 2. VALIDATION DATA\n",
        "# ----------------------------\n",
        "# Validation specifications\n",
        "url = \"https://drive.google.com/file/d/1tc-cAlepMP_EbObvxv0J6oqPH3u2DlLL/view?usp=sharing\"\n",
        "gdown.download(url, 'validation_specifications.csv', fuzzy=True, quiet=False)\n",
        "specs_val = pd.read_csv(\"validation_specifications.csv\")\n",
        "\n",
        "# Validation sensor data\n",
        "file_id = '1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'validation_operational_readouts.csv', quiet=False)\n",
        "ops_val = pd.read_csv(\"validation_operational_readouts.csv\")\n",
        "\n",
        "# Validation labels\n",
        "file_id = '1lnd0VkDb2234csyl1OGt2jecUx6KPbuw'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'validation_labels.csv', quiet=False)\n",
        "labels_val = pd.read_csv(\"validation_labels.csv\")\n",
        "# ----------------------------\n",
        "# 3. Merge val data\n",
        "# ----------------------------\n",
        "val_df = ops_val.merge(labels_val, on='vehicle_id').merge(specs_val, on='vehicle_id')\n",
        "val_df.dropna(inplace=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. TEST DATA\n",
        "# ----------------------------\n",
        "# Test specifications (corrected)\n",
        "file_id = '138drtUXNmG4dwrJgIESUNZuW0aHKEPJU'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'test_specifications.csv', quiet=False)\n",
        "specs_test = pd.read_csv(\"test_specifications.csv\")\n",
        "\n",
        "# Test sensor data (critical fix: new file ID)\n",
        "file_id = '1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E'  # New correct file ID\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'test_operational_readouts.csv', quiet=False)\n",
        "ops_test = pd.read_csv(\"test_operational_readouts.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Test labels (ADD THIS SECTION)\n",
        "file_id = '15njyTfKdErU3cvd837nuFoMBIgTtOOgv'  # Replace with actual file ID\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'test_labels.csv', quiet=False)\n",
        "labels_test = pd.read_csv(\"test_labels.csv\")\n",
        "#Merginf test_data\n",
        "test_df = ops_test.merge(labels_test, on='vehicle_id').merge(specs_test, on='vehicle_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# after processor.load_data() and processor.process_classes(...)\n",
        "print(\"Train nulls per column:\\n\", train_df.isna().sum())\n",
        "print(\"Val   nulls per column:\\n\", val_df.isna().sum())\n",
        "print(\"Test  nulls per column:\\n\", test_df.isna().sum())\n",
        "\n",
        "# also check your sequence arrays\n",
        "#print(\"X_train contains NaN?\", np.isnan(X_train).any())\n",
        "#print(\"X_val   contains NaN?\", np.isnan(X_val).any())\n",
        "#print(\"X_test  contains NaN?\", np.isnan(X_test).any())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itSCGXjhLbMZ",
        "outputId": "102abe8d-3315-45e8-a2a2-b7a3385db783"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train nulls per column:\n",
            " vehicle_id       0\n",
            "time_step        0\n",
            "171_0            0\n",
            "666_0           40\n",
            "427_0         6405\n",
            "              ... \n",
            "Spec_3           0\n",
            "Spec_4           0\n",
            "Spec_5           0\n",
            "Spec_6           0\n",
            "Spec_7           0\n",
            "Length: 117, dtype: int64\n",
            "Val   nulls per column:\n",
            " vehicle_id    0\n",
            "time_step     0\n",
            "171_0         0\n",
            "666_0         0\n",
            "427_0         0\n",
            "             ..\n",
            "Spec_3        0\n",
            "Spec_4        0\n",
            "Spec_5        0\n",
            "Spec_6        0\n",
            "Spec_7        0\n",
            "Length: 116, dtype: int64\n",
            "Test  nulls per column:\n",
            " vehicle_id       0\n",
            "time_step        0\n",
            "171_0            0\n",
            "666_0            8\n",
            "427_0         1025\n",
            "              ... \n",
            "Spec_3           0\n",
            "Spec_4           0\n",
            "Spec_5           0\n",
            "Spec_6           0\n",
            "Spec_7           0\n",
            "Length: 116, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIlGIScwKSXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c901a0-5d57-473f-8fec-5d11b9c3efd2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-e576a7345e4d>:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.ffill().bfill())\n",
            "<ipython-input-6-e576a7345e4d>:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.ffill().bfill())\n",
            "<ipython-input-6-e576a7345e4d>:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.ffill().bfill())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.3365 | Val Loss: 0.4375 | Val Acc: 0.8799\n",
            "Epoch 02 | Train Loss: 0.1912 | Val Loss: 0.7786 | Val Acc: 0.8406\n",
            "Epoch 03 | Train Loss: 0.1316 | Val Loss: 0.7809 | Val Acc: 0.8486\n",
            "Epoch 04 | Train Loss: 0.1072 | Val Loss: 0.9239 | Val Acc: 0.8278\n",
            "Epoch 05 | Train Loss: 0.0947 | Val Loss: 0.7234 | Val Acc: 0.8822\n",
            "Epoch 06 | Train Loss: 0.0864 | Val Loss: 0.9594 | Val Acc: 0.8304\n",
            "Epoch 07 | Train Loss: 0.0825 | Val Loss: 0.9516 | Val Acc: 0.8601\n",
            "Epoch 08 | Train Loss: 0.0495 | Val Loss: 1.4381 | Val Acc: 0.8422\n",
            "Epoch 09 | Train Loss: 0.0391 | Val Loss: 1.6509 | Val Acc: 0.8440\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# ========================\n",
        "# Hyperparameters\n",
        "# ========================\n",
        "SEQ_LEN = 20\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT = 0.3\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "COST_MATRIX = torch.tensor([\n",
        " # Pred→  0    1    2    3    4    5\n",
        "    [    0,   7,   8,   9,  10,  11],  # True=0\n",
        "    [  200,   0,   7,   8,   9,  10],  # True=1\n",
        "    [  300, 200,   0,   7,   8,   9],  # True=2\n",
        "    [  400, 300, 200,   0,   7,   8],  # True=3\n",
        "    [  500, 400, 300, 200,   0,   7],  # True=4\n",
        "    [  600, 500, 400, 300, 200,   0],  # True=5\n",
        "], dtype=torch.float32)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# 1. Enhanced Data Loading with Categorical Handling\n",
        "# ========================\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.encoder = OrdinalEncoder(\n",
        "            handle_unknown='use_encoded_value',\n",
        "            unknown_value=-1\n",
        "        )\n",
        "        self.fitted = False\n",
        "        self.spec_cols = [f'Spec_{i}' for i in range(8)]\n",
        "        self.sensor_prefixes = (\"167_\", \"272_\", \"459_\")\n",
        "\n",
        "    def load_data(self):\n",
        "        def load_and_merge(spec_path, ops_path, tte_path=None, label_path=None):\n",
        "            specs = pd.read_csv(spec_path)\n",
        "            ops   = pd.read_csv(ops_path)\n",
        "\n",
        "            # (optional) fill NaNs in your time series:\n",
        "            ops = (\n",
        "                ops.sort_values(['vehicle_id','time_step'])\n",
        "                   .groupby('vehicle_id')\n",
        "                   .apply(lambda g: g.ffill().bfill())\n",
        "                   .reset_index(drop=True)\n",
        "            )\n",
        "\n",
        "            # encode specs\n",
        "            if not self.fitted:\n",
        "                specs[self.spec_cols] = self.encoder.fit_transform(\n",
        "                    specs[self.spec_cols]\n",
        "                )\n",
        "                self.fitted = True\n",
        "            else:\n",
        "                specs[self.spec_cols] = self.encoder.transform(\n",
        "                    specs[self.spec_cols]\n",
        "                )\n",
        "\n",
        "            # merge in either raw TTE (for train) or pre-binned labels (for val/test)\n",
        "            if tte_path is not None:\n",
        "                tte = pd.read_csv(tte_path)\n",
        "                df  = ops.merge(tte, on='vehicle_id').merge(specs, on='vehicle_id')\n",
        "            elif label_path is not None:\n",
        "                lbl = pd.read_csv(label_path)\n",
        "                df  = ops.merge(lbl, on='vehicle_id').merge(specs, on='vehicle_id')\n",
        "            else:\n",
        "                df  = ops.merge(specs, on='vehicle_id')\n",
        "\n",
        "            df.dropna(inplace=True)\n",
        "            return df\n",
        "\n",
        "        train_df = load_and_merge(\n",
        "            \"train_specifications.csv\",\n",
        "            \"train_operational_readouts.csv\",\n",
        "            tte_path=\"train_tte.csv\"\n",
        "        )\n",
        "        val_df   = load_and_merge(\n",
        "            \"validation_specifications.csv\",\n",
        "            \"validation_operational_readouts.csv\",\n",
        "            label_path=\"validation_labels.csv\"\n",
        "        )\n",
        "        test_df  = load_and_merge(\n",
        "            \"test_specifications.csv\",\n",
        "            \"test_operational_readouts.csv\",\n",
        "            label_path=\"test_labels.csv\"\n",
        "        )\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    def process_classes(self, df, is_training=True):\n",
        "        if is_training:\n",
        "            # recompute 6-bucket labels from raw TTE\n",
        "            df['tte'] = df['length_of_study_time_step'] - df['time_step']\n",
        "            bins   = [-np.inf, 0, 6, 12, 24, 48, np.inf]\n",
        "            labels = [0,    5,  4,   3,   2,   1]\n",
        "            df['class_label'] = (\n",
        "                pd.cut(df['tte'], bins=bins, labels=labels)\n",
        "                  .astype(int)\n",
        "                  .where(df['in_study_repair']==1, 0)\n",
        "            )\n",
        "            df.drop(columns=['tte'], inplace=True)\n",
        "        else:\n",
        "            # val/test already have `class_label` 0–5\n",
        "            df['class_label'] = df['class_label'].astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "# Initialize data processor\n",
        "processor = DataProcessor()\n",
        "train_df, val_df, test_df = processor.load_data()\n",
        "\n",
        "train_df = processor.process_classes(train_df, is_training=True)\n",
        "val_df   = processor.process_classes(val_df,   is_training=False)\n",
        "test_df  = processor.process_classes(test_df,  is_training=False)\n",
        "\n",
        "#X_train, y_train = prepare_data(train_df)\n",
        "#X_val,   y_val   = prepare_data(val_df)\n",
        "#X_test,  y_test  = prepare_data(test_df)\n",
        "\n",
        "\n",
        "\n",
        "# ========================\n",
        "# 2. Optimized Sequence Generation\n",
        "# ========================\n",
        "def prepare_data(df, seq_len=20):\n",
        "    # Feature engineering with vectorized operations\n",
        "    sensor_cols = [c for c in df.columns if c.startswith(processor.sensor_prefixes)]\n",
        "    feature_cols = sensor_cols + processor.spec_cols\n",
        "\n",
        "    # Generate sequences using sliding window approach\n",
        "    groups = df.groupby('vehicle_id')\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for _, group in groups:\n",
        "        group_data = group[feature_cols].values.astype(np.float32)\n",
        "        if len(group_data) >= seq_len:\n",
        "            # Create sliding windows using NumPy\n",
        "            for i in range(len(group_data) - seq_len + 1):\n",
        "                window = group_data[i:i+seq_len]\n",
        "                sequences.append(window)\n",
        "                labels.append(group['class_label'].iloc[i+seq_len-1])\n",
        "\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Generate and scale datasets\n",
        "X_train, y_train = prepare_data(train_df)\n",
        "X_val, y_val = prepare_data(val_df)\n",
        "X_test, y_test = prepare_data(test_df)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "# ========================\n",
        "# 3. Enhanced Model Architecture\n",
        "# ========================\n",
        "class PredictiveMaintenanceModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, HIDDEN_DIM,\n",
        "                         bidirectional=True,\n",
        "                         num_layers=2,\n",
        "                         dropout=DROPOUT,\n",
        "                         batch_first=True)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM*2, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1))\n",
        "        #self.classifier = nn.Sequential(\n",
        "         #   nn.Linear(HIDDEN_DIM*2, 64),\n",
        "          #  nn.ReLU(),\n",
        "           # nn.Dropout(DROPOUT),\n",
        "            #nn.Linear(64, 5))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_DIM*2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT),\n",
        "            nn.Linear(64, 6))   # <-- now output 6 logits for classes 0–5\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        attn_weights = torch.softmax(self.attention(out), dim=1)\n",
        "        context = torch.sum(attn_weights * out, dim=1)\n",
        "        return self.classifier(context)\n",
        "\n",
        "model = PredictiveMaintenanceModel(X_train.shape[-1])\n",
        "\n",
        "# ========================\n",
        "# 4. Cost-Sensitive Training\n",
        "# ========================\n",
        "class CostAwareLoss(nn.Module):\n",
        "    def __init__(self, cost_matrix):\n",
        "        super().__init__()\n",
        "        self.cost_matrix = cost_matrix\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
        "        preds = torch.argmax(inputs, dim=1)\n",
        "        costs = self.cost_matrix[targets, preds]\n",
        "        return torch.mean(ce_loss * costs)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "#criterion = CostAwareLoss(COST_MATRIX)\n",
        "scaled_cost = COST_MATRIX / 100.0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ========================\n",
        "# 5. Training Loop with Early Stopping\n",
        "# ========================\n",
        "def create_loader(X, y, batch_size=64, shuffle=False):\n",
        "    dataset = TensorDataset(\n",
        "        torch.tensor(X, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                      pin_memory=True, num_workers=4)\n",
        "\n",
        "train_loader = create_loader(X_train, y_train, shuffle=True)\n",
        "val_loader = create_loader(X_val, y_val)\n",
        "test_loader = create_loader(X_test, y_test)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch)\n",
        "            val_loss += criterion(outputs, y_batch).item()\n",
        "            correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / len(val_loader.dataset)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# ========================\n",
        "# 6. Final Evaluation\n",
        "# ========================\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        all_preds.extend(outputs.argmax(1).tolist())\n",
        "        all_labels.extend(y_batch.tolist())\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "bucket_names = [\n",
        "    \"Healthy (tte ≤ 0)\",    # 0\n",
        "    \"> 48h remaining\",      # 1\n",
        "    \"24–48h remaining\",     # 2\n",
        "    \"12–24h remaining\",     # 3\n",
        "    \"6–12h remaining\",      # 4\n",
        "    \"0–6h remaining\"        # 5\n",
        "]\n",
        "\n",
        "# Ensure we report on all 6 buckets, even if some are missing in this run\n",
        "labels = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    labels=labels,\n",
        "    target_names=bucket_names,\n",
        "    zero_division=0  # avoids errors if a class has 0 predictions\n",
        "))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    labels=labels\n",
        "))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.pth"
      ],
      "metadata": {
        "id": "_Cm-OFcCVivk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "xRvwNtXEU38O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyMmoAaTSgACLqB+9WCw5x1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}