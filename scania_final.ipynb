{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLrppvJtdZiStUSr2lA/sh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakim733/scania-predictive-maintenance/blob/main/scania_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PckWiCX3J0vD",
        "outputId": "6e01d540-bf6c-4b18-8ec6-23c39cc2e5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18A955gJOHf0Ay3UComqy3SigMjf2X3Pi\n",
            "To: /content/train_specifications.csv\n",
            "100%|██████████| 1.08M/1.08M [00:00<00:00, 10.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WG7FYf9KaidtOaM04gBubkwYLi292Ken\n",
            "To: /content/train_tte.csv\n",
            "100%|██████████| 345k/345k [00:00<00:00, 5.74MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp\n",
            "From (redirected): https://drive.google.com/uc?id=1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp&confirm=t&uuid=825bc4a4-5336-4f73-a861-75924b64880e\n",
            "To: /content/train_operational_readouts.csv\n",
            "100%|██████████| 1.22G/1.22G [00:25<00:00, 48.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tc-cAlepMP_EbObvxv0J6oqPH3u2DlLL\n",
            "To: /content/validation_specifications.csv\n",
            "100%|██████████| 232k/232k [00:00<00:00, 4.32MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc\n",
            "From (redirected): https://drive.google.com/uc?id=1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc&confirm=t&uuid=7d38b5cc-0f62-4d00-80f1-0df17bff10bd\n",
            "To: /content/validation_operational_readouts.csv\n",
            "100%|██████████| 216M/216M [00:01<00:00, 134MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lnd0VkDb2234csyl1OGt2jecUx6KPbuw\n",
            "To: /content/validation_labels.csv\n",
            "100%|██████████| 38.7k/38.7k [00:00<00:00, 7.87MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=138drtUXNmG4dwrJgIESUNZuW0aHKEPJU\n",
            "To: /content/test_specifications.csv\n",
            "100%|██████████| 232k/232k [00:00<00:00, 4.92MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E\n",
            "From (redirected): https://drive.google.com/uc?id=1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E&confirm=t&uuid=529af9b4-317a-4acc-acc7-dd69fe696614\n",
            "To: /content/test_operational_readouts.csv\n",
            "100%|██████████| 215M/215M [00:02<00:00, 106MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# 1. TRAINING DATA\n",
        "# ----------------------------\n",
        "# Training specifications\n",
        "file_id = '18A955gJOHf0Ay3UComqy3SigMjf2X3Pi'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_specifications.csv', quiet=False)\n",
        "train_specs = pd.read_csv(\"train_specifications.csv\")\n",
        "\n",
        "# Training labels (time-to-event)\n",
        "file_id = '1WG7FYf9KaidtOaM04gBubkwYLi292Ken'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_tte.csv', quiet=False)\n",
        "train_tte = pd.read_csv(\"train_tte.csv\")[['vehicle_id', 'in_study_repair','length_of_study_time_step']]\n",
        "\n",
        "# Training sensor data (critical fix: corrected file ID)\n",
        "file_id = '1zBeLuuyEB1SOc_Fr06khtC090-cWFQGp'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'train_operational_readouts.csv', quiet=False)\n",
        "train_ops = pd.read_csv(\"train_operational_readouts.csv\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. VALIDATION DATA\n",
        "# ----------------------------\n",
        "# Validation specifications\n",
        "url = \"https://drive.google.com/file/d/1tc-cAlepMP_EbObvxv0J6oqPH3u2DlLL/view?usp=sharing\"\n",
        "gdown.download(url, 'validation_specifications.csv', fuzzy=True, quiet=False)\n",
        "specs_val = pd.read_csv(\"validation_specifications.csv\")\n",
        "\n",
        "# Validation sensor data\n",
        "file_id = '1cLKKPAP7sZRE0SxjFc7wGX_AfARcaHGc'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'validation_operational_readouts.csv', quiet=False)\n",
        "ops_val = pd.read_csv(\"validation_operational_readouts.csv\")\n",
        "\n",
        "# Validation labels\n",
        "file_id = '1lnd0VkDb2234csyl1OGt2jecUx6KPbuw'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'validation_labels.csv', quiet=False)\n",
        "labels_val = pd.read_csv(\"validation_labels.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3. TEST DATA\n",
        "# ----------------------------\n",
        "# Test specifications (corrected)\n",
        "file_id = '138drtUXNmG4dwrJgIESUNZuW0aHKEPJU'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'test_specifications.csv', quiet=False)\n",
        "specs_test = pd.read_csv(\"test_specifications.csv\")\n",
        "\n",
        "# Test sensor data (critical fix: new file ID)\n",
        "file_id = '1djLlBBTTyh4l4DmW_TrTtQ6P8HpTl26E'  # New correct file ID\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'test_operational_readouts.csv', quiet=False)\n",
        "ops_test = pd.read_csv(\"test_operational_readouts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# --- Parameters ---\n",
        "SEQ_LEN = 10\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_DIM = 128\n",
        "EPOCHS = 10\n",
        "spec_cols = [f\"Spec_{i}\" for i in range(8)]\n",
        "cost_matrix = np.array([\n",
        "    [0,   9,   10],\n",
        "    [300, 0,   8],\n",
        "    [500, 300, 0]\n",
        "])\n",
        "\n",
        "# ========================\n",
        "# 1. Data Preprocessing\n",
        "# ========================\n",
        "df = train_ops.merge(train_tte, on='vehicle_id')\n",
        "df = df.merge(train_specs, on='vehicle_id')\n",
        "df.dropna(subset=['in_study_repair', 'length_of_study_time_step', 'time_step'], inplace=True)\n",
        "\n",
        "# --- Compute TTE + Label ---\n",
        "df['tte'] = df['length_of_study_time_step'] - df['time_step']\n",
        "conditions = [\n",
        "    (df['in_study_repair'] == 1) & (df['tte'] > 48),                     # class 1\n",
        "    (df['in_study_repair'] == 1) & (df['tte'] > 24) & (df['tte'] <= 48), # class 2\n",
        "    (df['in_study_repair'] == 1) & (df['tte'] > 12) & (df['tte'] <= 24), # class 3\n",
        "    (df['in_study_repair'] == 1) & (df['tte'] > 6)  & (df['tte'] <= 12), # class 4\n",
        "    (df['in_study_repair'] == 1) & (df['tte'] >= 0) & (df['tte'] <= 6),  # class 5\n",
        "]\n",
        "df['class_label'] = np.select(conditions, [1, 2, 3, 4, 5], default=0)\n",
        "\n",
        "# --- Merge rare classes 1–3 into 4 due to ambalanced sampling ---\n",
        "df['class_label'] = df['class_label'].replace({1: 4, 2: 4, 3: 4})\n",
        "num_classes = 3  # final labels: 0, 4, 5\n",
        "# Map class labels to 0, 1, 2 for CrossEntropy compatibility\n",
        "label_map = {0: 0, 4: 1, 5: 2}\n",
        "df['class_label'] = df['class_label'].map(label_map)\n",
        "\n",
        "# --- Encode specs ---\n",
        "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "df[spec_cols] = enc.fit_transform(df[spec_cols])\n",
        "df[spec_cols] = df[spec_cols].astype(float)\n",
        "\n",
        "# --- Feature scaling ---\n",
        "sensor_cols = [c for c in df.columns if c.startswith(\"Sensor_\")]\n",
        "feature_cols = sensor_cols + spec_cols\n",
        "scaler = StandardScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "\n",
        "\n",
        "# Optional: shuffle\n",
        "from sklearn.utils import shuffle\n",
        "X_balanced, y_balanced = shuffle(X_balanced, y_balanced, random_state=42)\n",
        "\n",
        "# 4. Replace in the DataLoader\n",
        "tX = torch.tensor(X_balanced, dtype=torch.float32)\n",
        "ty = torch.tensor(y_balanced, dtype=torch.long)\n",
        "train_ds = TensorDataset(tX, ty)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# --- Sequence building ---\n",
        "sequences, labels = [], []\n",
        "for _, group in df.sort_values('time_step').groupby('vehicle_id'):\n",
        "    if len(group) >= SEQ_LEN:\n",
        "        seq = group[feature_cols].values[-SEQ_LEN:]\n",
        "        label = group['class_label'].iloc[-1]\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "\n",
        "X_train = np.array(sequences)\n",
        "y_train = np.array(labels)\n",
        "\n",
        "tX = torch.tensor(X_train, dtype=torch.float32)\n",
        "ty = torch.tensor(y_train, dtype=torch.long)\n",
        "train_ds = TensorDataset(tX, ty)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# --- Upsampling ---\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# 1. Separate the data by class\n",
        "X_majority = X_train[y_train == 0]\n",
        "X_class1   = X_train[y_train == 1]\n",
        "X_class2   = X_train[y_train == 2]\n",
        "\n",
        "y_majority = y_train[y_train == 0]\n",
        "y_class1   = y_train[y_train == 1]\n",
        "y_class2   = y_train[y_train == 2]\n",
        "\n",
        "# 2. Upsample minority classes to match class 0\n",
        "X_class1_upsampled, y_class1_upsampled = resample(\n",
        "    X_class1, y_class1,\n",
        "    replace=True,\n",
        "    n_samples=len(X_majority),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_class2_upsampled, y_class2_upsampled = resample(\n",
        "    X_class2, y_class2,\n",
        "    replace=True,\n",
        "    n_samples=len(X_majority),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# 3. Combine all into one balanced dataset\n",
        "X_balanced = np.vstack((X_majority, X_class1_upsampled, X_class2_upsampled))\n",
        "y_balanced = np.hstack((y_majority, y_class1_upsampled, y_class2_upsampled))\n",
        "# ========================\n",
        "# 2. GRU Model\n",
        "# ========================\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, h_n = self.gru(x)\n",
        "        return self.fc(h_n.squeeze(0))\n",
        "\n",
        "model = GRUClassifier(input_dim=tX.shape[2], hidden_dim=HIDDEN_DIM, num_classes=num_classes)\n",
        "\n",
        "# ========================\n",
        "# 3. Loss & Optimizer\n",
        "# ========================\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = self.ce(input, target)\n",
        "        p = torch.exp(-logp)\n",
        "        loss = (1 - p) ** self.gamma * logp\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "criterion = FocalLoss(gamma=2.0, weight=torch.tensor(class_weights, dtype=torch.float32))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ========================\n",
        "# 4. Training Loop\n",
        "# ========================\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on train (placeholder)\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in train_loader:\n",
        "            out = model(xb)\n",
        "            pred = torch.argmax(out, dim=1).cpu().numpy()\n",
        "            label = yb.cpu().numpy()\n",
        "            all_preds.extend(pred)\n",
        "            all_labels.extend(label)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nEpoch {epoch+1} — Acc: {acc:.4f}\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIlGIScwKSXG",
        "outputId": "b66031b9-0191-43f6-e015-8db61a756b4b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 — Acc: 0.5374\n",
            "[[10981    79  9270]\n",
            " [   61     1   183]\n",
            " [  826    13  1139]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9253    0.5401    0.6821     20330\n",
            "           1     0.0108    0.0041    0.0059       245\n",
            "           2     0.1075    0.5758    0.1812      1978\n",
            "\n",
            "    accuracy                         0.5374     22553\n",
            "   macro avg     0.3478    0.3734    0.2897     22553\n",
            "weighted avg     0.8436    0.5374    0.6308     22553\n",
            "\n",
            "\n",
            "Epoch 2 — Acc: 0.5632\n",
            "[[11716   339  8275]\n",
            " [   75    21   149]\n",
            " [  950    63   965]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9196    0.5763    0.7085     20330\n",
            "           1     0.0496    0.0857    0.0629       245\n",
            "           2     0.1028    0.4879    0.1698      1978\n",
            "\n",
            "    accuracy                         0.5632     22553\n",
            "   macro avg     0.3573    0.3833    0.3137     22553\n",
            "weighted avg     0.8385    0.5632    0.6543     22553\n",
            "\n",
            "\n",
            "Epoch 3 — Acc: 0.5238\n",
            "[[10767   640  8923]\n",
            " [   69    27   149]\n",
            " [  871    87  1020]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9197    0.5296    0.6722     20330\n",
            "           1     0.0358    0.1102    0.0541       245\n",
            "           2     0.1011    0.5157    0.1690      1978\n",
            "\n",
            "    accuracy                         0.5238     22553\n",
            "   macro avg     0.3522    0.3852    0.2984     22553\n",
            "weighted avg     0.8383    0.5238    0.6213     22553\n",
            "\n",
            "\n",
            "Epoch 4 — Acc: 0.5822\n",
            "[[12111   560  7659]\n",
            " [   82    22   141]\n",
            " [  900    81   997]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9250    0.5957    0.7247     20330\n",
            "           1     0.0332    0.0898    0.0485       245\n",
            "           2     0.1133    0.5040    0.1851      1978\n",
            "\n",
            "    accuracy                         0.5822     22553\n",
            "   macro avg     0.3572    0.3965    0.3194     22553\n",
            "weighted avg     0.8441    0.5822    0.6700     22553\n",
            "\n",
            "\n",
            "Epoch 5 — Acc: 0.5721\n",
            "[[11842   268  8220]\n",
            " [   84    21   140]\n",
            " [  887    51  1040]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9242    0.5825    0.7146     20330\n",
            "           1     0.0618    0.0857    0.0718       245\n",
            "           2     0.1106    0.5258    0.1828      1978\n",
            "\n",
            "    accuracy                         0.5721     22553\n",
            "   macro avg     0.3655    0.3980    0.3231     22553\n",
            "weighted avg     0.8435    0.5721    0.6610     22553\n",
            "\n",
            "\n",
            "Epoch 6 — Acc: 0.5533\n",
            "[[11382   307  8641]\n",
            " [   87    25   133]\n",
            " [  853    53  1072]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9237    0.5599    0.6972     20330\n",
            "           1     0.0649    0.1020    0.0794       245\n",
            "           2     0.1089    0.5420    0.1813      1978\n",
            "\n",
            "    accuracy                         0.5533     22553\n",
            "   macro avg     0.3658    0.4013    0.3193     22553\n",
            "weighted avg     0.8429    0.5533    0.6452     22553\n",
            "\n",
            "\n",
            "Epoch 7 — Acc: 0.4901\n",
            "[[ 9804   308 10218]\n",
            " [   68    25   152]\n",
            " [  692    62  1224]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9281    0.4822    0.6347     20330\n",
            "           1     0.0633    0.1020    0.0781       245\n",
            "           2     0.1056    0.6188    0.1804      1978\n",
            "\n",
            "    accuracy                         0.4901     22553\n",
            "   macro avg     0.3656    0.4010    0.2977     22553\n",
            "weighted avg     0.8465    0.4901    0.5888     22553\n",
            "\n",
            "\n",
            "Epoch 8 — Acc: 0.6097\n",
            "[[12795   378  7157]\n",
            " [   94    26   125]\n",
            " [  978    71   929]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9227    0.6294    0.7483     20330\n",
            "           1     0.0547    0.1061    0.0722       245\n",
            "           2     0.1131    0.4697    0.1824      1978\n",
            "\n",
            "    accuracy                         0.6097     22553\n",
            "   macro avg     0.3635    0.4017    0.3343     22553\n",
            "weighted avg     0.8423    0.6097    0.6913     22553\n",
            "\n",
            "\n",
            "Epoch 9 — Acc: 0.6056\n",
            "[[12973  3291  4066]\n",
            " [   98    62    85]\n",
            " [  999   355   624]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9220    0.6381    0.7542     20330\n",
            "           1     0.0167    0.2531    0.0314       245\n",
            "           2     0.1307    0.3155    0.1848      1978\n",
            "\n",
            "    accuracy                         0.6056     22553\n",
            "   macro avg     0.3565    0.4022    0.3235     22553\n",
            "weighted avg     0.8428    0.6056    0.6964     22553\n",
            "\n",
            "\n",
            "Epoch 10 — Acc: 0.5175\n",
            "[[10383   253  9694]\n",
            " [   67    22   156]\n",
            " [  660    52  1266]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9346    0.5107    0.6605     20330\n",
            "           1     0.0673    0.0898    0.0769       245\n",
            "           2     0.1139    0.6400    0.1934      1978\n",
            "\n",
            "    accuracy                         0.5175     22553\n",
            "   macro avg     0.3719    0.4135    0.3103     22553\n",
            "weighted avg     0.8532    0.5175    0.6132     22553\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CQRr7zVbIJW",
        "outputId": "c2445dc3-e71e-4277-dbf6-a6c94a13319a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Accuracy: 0.4388 | Cost: 24,655\n",
            "Confusion Matrix:\n",
            " [[2033   35 2434]\n",
            " [   0    0    0]\n",
            " [   0    0    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9779    0.4516    0.6178      4502\n",
            "           1     0.0000    0.0000    0.0000         0\n",
            "           2     0.0000    0.0000    0.0000         0\n",
            "           4     0.0000    0.0000    0.0000       131\n",
            "\n",
            "    accuracy                         0.4388      4633\n",
            "   macro avg     0.2445    0.1129    0.1545      4633\n",
            "weighted avg     0.9502    0.4388    0.6004      4633\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_eval_data(ops_df, specs_df, labels_df=None):\n",
        "    # Merge operational + specs\n",
        "    df = ops_df.merge(specs_df, on='vehicle_id', how='left')\n",
        "    if labels_df is not None:\n",
        "        df = df.merge(labels_df, on='vehicle_id', how='left')\n",
        "\n",
        "    # Apply encoder + scaler\n",
        "    df[spec_cols] = enc.transform(df[spec_cols])\n",
        "    df[spec_cols] = df[spec_cols].astype(float)\n",
        "    df[feature_cols] = scaler.transform(df[feature_cols])\n",
        "\n",
        "    # If class_label exists, re-map classes\n",
        "    if 'class_label' in df.columns:\n",
        "        df['class_label'] = df['class_label'].replace({1: 4, 2: 4, 3: 4})\n",
        "        df['class_label'] = df['class_label'].map({0: 0, 4: 1, 5: 2})\n",
        "\n",
        "    # Build sequences\n",
        "    sequences, labels = [], []\n",
        "    for _, group in df.sort_values('time_step').groupby('vehicle_id'):\n",
        "        if len(group) >= SEQ_LEN:\n",
        "            seq = group[feature_cols].values[-SEQ_LEN:]\n",
        "            sequences.append(seq)\n",
        "            if 'class_label' in group:\n",
        "                labels.append(group['class_label'].iloc[-1])\n",
        "\n",
        "    X_eval = np.array(sequences)\n",
        "    tX_eval = torch.tensor(X_eval, dtype=torch.float32)\n",
        "\n",
        "    if labels:\n",
        "        y_eval = np.array(labels)\n",
        "        ty_eval = torch.tensor(y_eval, dtype=torch.long)\n",
        "        ds = TensorDataset(tX_eval, ty_eval)\n",
        "    else:\n",
        "        ds = TensorDataset(tX_eval)\n",
        "\n",
        "    return ds\n",
        ""
      ],
      "metadata": {
        "id": "r1zpz7LEK7SG"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Validation set\n",
        "val_ds = process_eval_data(ops_val, specs_val , labels_val)\n",
        "val_loader = DataLoader(val_ds, batch_size=200, shuffle=False)\n",
        "\n",
        "# Test set (no labels)\n",
        "test_ds = process_eval_data(specs_test, ops_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=200, shuffle=False)\n"
      ],
      "metadata": {
        "id": "2jfsLHPBce89"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Validation ---\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        out = model(xb.to(model.fc.weight.device))\n",
        "        preds = torch.argmax(out, dim=1).cpu().numpy()\n",
        "        labels = yb.cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nEpoch {epoch+1} — Val Acc: {acc:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds, labels=[0, 1, 2]))\n",
        "print(classification_report(all_labels, all_preds, labels=[0, 1, 2], digits=4, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwb4zx9Jc8N_",
        "outputId": "5a5b3956-89cd-4c92-a376-1385bb1c8172"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 — Val Acc: 0.4399\n",
            "Confusion Matrix:\n",
            "[[2033   35 2434]\n",
            " [  46    5   80]\n",
            " [   0    0    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9779    0.4516    0.6178      4502\n",
            "           1     0.1250    0.0382    0.0585       131\n",
            "           2     0.0000    0.0000    0.0000         0\n",
            "\n",
            "    accuracy                         0.4399      4633\n",
            "   macro avg     0.3676    0.1632    0.2254      4633\n",
            "weighted avg     0.9538    0.4399    0.6020      4633\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate test predictions\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb[0].to(model.fc.weight.device)\n",
        "        out = model(xb)\n",
        "        preds = torch.argmax(out, dim=1).cpu().numpy()\n",
        "        test_preds.extend(preds)\n",
        "\n",
        "# Map back to original labels (reverse of label_map: {0: 0, 4: 1, 5: 2})\n",
        "inv_label_map = {0: 0, 1: 4, 2: 5}\n",
        "final_preds = [inv_label_map[p] for p in test_preds]\n",
        "\n",
        "# Print first few predictions\n",
        "print(\"First 30 test predictions:\")\n",
        "print(final_preds[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sYQX96udmwo",
        "outputId": "18303753-cb98-4f4f-99a3-fe454a07f5d6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 30 test predictions:\n",
            "[5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5]\n"
          ]
        }
      ]
    }
  ]
}